import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.biases_hidden = np.zeros((1, hidden_size))
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.biases_output = np.zeros((1, output_size))

    def forward(self, inputs):
        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.biases_hidden
        self.hidden_layer_output = sigmoid(self.hidden_layer_input)
        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.biases_output
        self.predicted_output = sigmoid(self.output_layer_input)
        return self.predicted_output

    def backward(self, inputs, targets, learning_rate):
        error = targets - self.predicted_output

        output_delta = error * sigmoid_derivative(self.predicted_output)
        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)
	#*****************
        omega = 0.9
        epsilon = 1e-8

        cache_weights_hidden_output = np.zeros_like(self.weights_hidden_output)
        cache_biases_output = np.zeros_like(self.biases_output)
        cache_weights_input_hidden = np.zeros_like(self.weights_input_hidden)
        cache_biases_hidden = np.zeros_like(self.biases_hidden)

        cache_weights_hidden_output = omega * cache_weights_hidden_output + (1 - omega) * np.square(self.hidden_layer_output.T.dot(output_delta))
        cache_biases_output = omega * cache_biases_output + (1 - omega) * np.square(np.sum(output_delta, axis=0, keepdims=True))

        self.weights_hidden_output += (learning_rate / np.sqrt(cache_weights_hidden_output + epsilon)) * self.hidden_layer_output.T.dot(output_delta)
        self.biases_output += (learning_rate / np.sqrt(cache_biases_output + epsilon)) * np.sum(output_delta, axis=0, keepdims=True)

        cache_weights_input_hidden = omega * cache_weights_input_hidden + (1 - omega) * np.square(inputs.T.dot(hidden_delta))
        cache_biases_hidden = omega * cache_biases_hidden + (1 - omega) * np.square(np.sum(hidden_delta, axis=0, keepdims=True))

        self.weights_input_hidden += (learning_rate / np.sqrt(cache_weights_input_hidden + epsilon)) * inputs.T.dot(hidden_delta)
        self.biases_hidden += (learning_rate / np.sqrt(cache_biases_hidden + epsilon)) * np.sum(hidden_delta, axis=0, keepdims=True)

    def train(self, inputs, targets, epochs, learning_rate):
        loss_history = []

        for epoch in range(epochs):
            predicted_output = self.forward(inputs)
            loss = np.mean(0.5 * (targets - predicted_output) ** 2)
            loss_history.append(loss)
            self.backward(inputs, targets, learning_rate)

            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss}")

        return loss_history

np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 2 + 3 * X**2 + np.random.rand(100, 1)

#X_normalized = (X - X.mean()) / X.std()
#y_normalized = (y - y.mean()) / y.std()

X_normalized = (X - X.min()) / (X.max() - X.min())
y_normalized = (y - y.min()) / (y.max() - y.min())

input_size = 1
hidden_size = 4
output_size = 1
learning_rate = 0.03
epochs = 600

model = NeuralNetwork(input_size, hidden_size, output_size)
loss_history = model.train(X_normalized, y_normalized, epochs, learning_rate)

plt.plot(loss_history)
plt.title('GD Training Loss')
plt.show()

X_test = np.linspace(X_normalized.min(), X_normalized.max(), 100).reshape(-1, 1)
y_pred_gd = model.forward(X_test)

plt.figure(figsize=(10, 6))
plt.scatter(X_normalized, y_normalized, label='True Data')
plt.plot(X_test, y_pred_gd, label='RMSprop', linewidth=2)
plt.title('Neural Network Regression')
plt.xlabel('Normalized X')
plt.ylabel('Normalized y')
plt.legend()
plt.show()
