import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Napravimo klasu za NN mrežu
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        # Initialization of weights and biases
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.biases_hidden = np.zeros((1, hidden_size))
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.biases_output = np.zeros((1, output_size))

        # AdaGrad parameters
        #****************************************
        self.epsilon = 1e-8
        self.g_squared_weights_input_hidden = np.zeros_like(self.weights_input_hidden)
        self.g_squared_biases_hidden = np.zeros_like(self.biases_hidden)
        self.g_squared_weights_hidden_output = np.zeros_like(self.weights_hidden_output)
        self.g_squared_biases_output = np.zeros_like(self.biases_output)

    def forward(self, inputs):
        # Propagate inputs through the network
        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.biases_hidden
        self.hidden_layer_output = sigmoid(self.hidden_layer_input)

        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.biases_output
        self.predicted_output = sigmoid(self.output_layer_input)

        return self.predicted_output

    def backward(self, inputs, targets, learning_rate):
        # Backward pass and weight update using AdaGrad
        error = targets - self.predicted_output

        output_delta = error * sigmoid_derivative(self.predicted_output)
        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)

        # Update squared gradients for input to hidden layer
        self.g_squared_weights_input_hidden += np.square(inputs.T.dot(hidden_delta))
        self.g_squared_biases_hidden += np.square(np.sum(hidden_delta, axis=0, keepdims=True))

        # Update weights and biases for input to hidden layer using AdaGrad
        self.weights_input_hidden += (inputs.T.dot(hidden_delta) /
                                      (np.sqrt(self.g_squared_weights_input_hidden) + self.epsilon)) * learning_rate
        self.biases_hidden += (np.sum(hidden_delta, axis=0, keepdims=True) /
                              (np.sqrt(self.g_squared_biases_hidden) + self.epsilon)) * learning_rate

        # Update squared gradients for hidden to output layer
        self.g_squared_weights_hidden_output += np.square(self.hidden_layer_output.T.dot(output_delta))
        self.g_squared_biases_output += np.square(np.sum(output_delta, axis=0, keepdims=True))

        # Update weights and biases for hidden to output layer using AdaGrad
        self.weights_hidden_output += (self.hidden_layer_output.T.dot(output_delta) /
                                       (np.sqrt(self.g_squared_weights_hidden_output) + self.epsilon)) * learning_rate
        self.biases_output += (np.sum(output_delta, axis=0, keepdims=True) /
                              (np.sqrt(self.g_squared_biases_output) + self.epsilon)) * learning_rate

    def train(self, inputs, targets, epochs, learning_rate):
        loss_history = []

        for epoch in range(epochs):
            # Forward pass
            predicted_output = self.forward(inputs)

            # Calculate and store loss
            loss = np.mean(0.5 * (targets - predicted_output) ** 2)
            loss_history.append(loss)

            # Backward pass and weight update using AdaGrad
            self.backward(inputs, targets, learning_rate)

            # Print the loss every 100 epochs
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Loss: {loss}")

        return loss_history

# Podatke veštački generišemo, uzmemo neke vrednosti za x,
#izračunamo kolika bi bila vrednost izlaza iz funcije za te vrednosti dodamo malo "šuma" na to
# sa ovim np.random.randn
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 3 * X**2 + 8*X + 4 + np.random.randn(100, 1) #*****************************

# Normalizacija podataka
#X_normalized = (X - X.mean()) / X.std()
#y_normalized = (y - y.mean()) / y.std()

X_normalized = (X - X.min()) / (X.max() - X.min())
y_normalized = (y - y.min()) / (y.max() - y.min())

# Hiperparametri NN mreže, tj parametri njene arhitekture
#*************************************
input_size = 1
hidden_size = 5
output_size = 1
learning_rate = 0.03
epochs = 600

# Pravljenje modela mreže
model = NeuralNetwork(input_size, hidden_size, output_size)
loss_history = model.train(X_normalized, y_normalized, epochs, learning_rate)

# Prikaz kako se menjala greška izlata pri procesu obuke
plt.plot(loss_history)
plt.title('AdaGrad Training Loss')
plt.show()

# Testiranje mreže na podacima koje mreža "nije videla", tj nad kojima nije trenirana
X_test = np.linspace(X_normalized.min(), X_normalized.max(), 100).reshape(-1, 1)
y_pred_adagrad = model.forward(X_test)

# Uporedni prikaz stvarnih izlaznih podataka i onoga što je mreža predvidjela
plt.figure(figsize=(10, 6))
plt.scatter(X_normalized, y_normalized, label='True Data')
plt.plot(X_test, y_pred_adagrad, label='AdaGrad', linewidth=2)
plt.title('Neural Network Regression ')
plt.xlabel('Normalized X')
plt.ylabel('Normalized y')
plt.legend()
plt.show()
